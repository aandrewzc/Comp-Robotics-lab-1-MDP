p_error set to 0.25 temporarily for demo purpose

(Q 1a)
Number of states: 768
Some states: [(7, 7, 0), (1, 5, 11), (0, 3, 8), (4, 5, 9), (4, 7, 10)]

(Q 1b)
Number of actions: 7
Actions: [(0, 0), (1, 0), (1, -1), (1, 1), (-1, 0), (-1, -1), (-1, 1)]

(Q 1c)
Some P entries:-
P( (4, 0, 4) , (1, -1) , (4, 0, 4) ): 0.25
P( (7, 0, 9) , (-1, 1) , (7, 0, 11) ): 0.25
P( (4, 4, 4) , (-1, 0) , (3, 4, 3) ): 0.25
P( (1, 0, 2) , (1, -1) , (1, 1, 0) ): 0.25
P( (7, 4, 11) , (1, 1) , (7, 5, 0) ): 0.5

(Q 1d)
Some s,a,s' drawed according to P:-
s = (6, 6, 3) , a = (0, 0) : s' = (6, 6, 3)
s = (6, 6, 7) , a = (0, 0) : s' = (6, 6, 7)
s = (7, 4, 1) , a = (1, -1) : s' = (7, 5, 11)
s = (5, 3, 0) , a = (-1, 0) : s' = (5, 2, 1)
s = (1, 0, 1) , a = (1, -1) : s' = (1, 1, 11)

(Q 2a)
Some R entries:-
R( (3, 3, 1) ): 0
R( (6, 1, 6) ): 0
R( (1, 4, 2) ): 0
R( (0, 0, 4) ): -100
R( (3, 2, 10) ): 0

p_error set back to 0.0

Initializing the MDP with p_error = 0.0 ...
................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................
Done!

(Q 3a)
Initial policy's action for some states:-
s = (4, 2, 8) : a = (-1, -1)
s = (4, 2, 10) : a = (1, 1)
s = (0, 0, 3) : a = (1, -1)
s = (2, 1, 2) : a = (1, -1)
s = (4, 5, 7) : a = (-1, 1)

(Q 3b)
Demonstration of asked function using initial policy and (5,2,6) as start state:-
Simulating 200  time steps...

(Q 3c)
Trajectory using intial policy and (1,6,6) as start state:-
Simulating 200  time steps...

(Q 3d)
Performing policy evaluation...
Values of some states for the initial policy:-
s = (5, 4, 10) : V(s) = -729.0000000000001
s = (7, 5, 4) : V(s) = 5.684341886080805e-14
s = (6, 3, 6) : V(s) = -728.9999999999999
s = (2, 2, 5) : V(s) = -1.9755900000000084
s = (6, 6, 3) : V(s) = 9.999999999999991

Action values of some state-action pairs for the initial policy:-
s = (1, 4, 6) , a = (-1, -1) : Q(s,a) = -1.3851000000000042
s = (6, 1, 4) , a = (-1, 0) : Q(s,a) = 5.904899999999993
s = (0, 6, 4) , a = (1, -1) : Q(s,a) = -1.5390000000000046
s = (7, 6, 7) , a = (1, -1) : Q(s,a) = -410.78421052631603
s = (6, 0, 2) , a = (-1, -1) : Q(s,a) = -652.0739684736844

(Q 3e)
Cumulative reward for the trajectory in 3c: -485.58689929449224

(Q 3f)
Testing if this function returns the initial policy back:-
pi_0_new==pi_0? False
Max difference between values of two policies: 3.410605131648481e-13

(Q 3g)
Performing policy iteration...
.....
Actions and values for some states for the policy found by policy iteration:-
s = (7, 0, 5) : pi*[s] = (-1, -1) , V*[s] = -95.217031
s = (2, 5, 3) : pi*[s] = (-1, 1) , V*[s] = 3.8742048900000006
s = (1, 3, 4) : pi*[s] = (1, -1) , V*[s] = 5.314409999999997
s = (4, 4, 4) : pi*[s] = (1, 1) , V*[s] = 8.1
s = (6, 2, 7) : pi*[s] = (-1, 0) , V*[s] = 6.5609999999999955

(Q 3h)
Simulating 200  time steps...
Cumulative reward for the trajectory just shown: 3.874204882944923

(Q 3i)
Running timeit to measure running time of policy iteration:-
.....
.....
.....
Time taken by policy iteration: 0.35041848366666767 sec

(Q 4a)

.....................................................................................................................................
Actions and values for some states for the policy found by value iteration:-
s = (6, 0, 4) : pi*[s] = (-1, 1) , V*[s] = -94.68559912034456
s = (5, 2, 11) : pi*[s] = (1, 0) , V*[s] = 7.289990879655445
s = (1, 4, 2) : pi*[s] = (1, -1) , V*[s] = 4.782959879655444
s = (4, 5, 4) : pi*[s] = (1, 1) , V*[s] = 8.999990879655444
s = (2, 0, 3) : pi*[s] = (1, -1) , V*[s] = -185.69533702034457

(Q 4b)
Simulating 200  time steps...
Cumulative reward for the trajectory just shown: 3.874204882944923
PI and VI give policies with same cumulative reward
However, the policies we get from these are different

(Q 4c)
Running timeit to measure running time of value iteration:-
.....................................................................................................................................
.....................................................................................................................................
.....................................................................................................................................
Time taken by policy iteration: 3.2204585023333343 sec

Initializing the MDP with p_error = 0.25 ...
................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................
Done!

(Q 5a)
Performing policy iteration...
.........
Actions and values for some states for the policy found by policy iteration:-
s = (1, 7, 8) : pi*[s] = (-1, -1) , V*[s] = -149.29664675399235
s = (4, 7, 5) : pi*[s] = (1, 1) , V*[s] = -28.848044368314184
s = (6, 3, 6) : pi*[s] = (-1, -1) , V*[s] = 5.457885448848863
s = (4, 4, 8) : pi*[s] = (-1, -1) , V*[s] = 6.3230874346693495
s = (1, 5, 11) : pi*[s] = (-1, 0) , V*[s] = 2.095014339175097

Simulating 200  time steps...
(1, 6, 6) (1, 0) (1, 5, 5) 0
(1, 5, 5) (1, 0) (1, 4, 5) 0
(1, 4, 5) (1, -1) (2, 4, 3) 0
(2, 4, 3) (-1, 1) (1, 4, 4) 0
(1, 4, 4) (1, -1) (1, 3, 4) 0
(1, 3, 4) (1, -1) (2, 3, 3) 0
(2, 3, 3) (1, 0) (3, 3, 4) 0
(3, 3, 4) (1, -1) (3, 2, 4) 0
(3, 2, 4) (1, 0) (4, 2, 4) 0
(4, 2, 4) (1, 1) (5, 2, 4) 0
(5, 2, 4) (-1, 1) (5, 3, 6) 0
(5, 3, 6) (-1, 0) (5, 4, 6) 0
(5, 4, 6) (-1, 0) (5, 5, 6) 0
(5, 5, 6) (-1, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
(5, 6, 6) (0, 0) (5, 6, 6) 1
Cumulative reward for the trajectory just shown: 2.5418658212739227

(Q 5b)
Initializing the MDP with new reward function and p_error = 0.0 ...
................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................
Done!

Performing policy iteration...
......
Actions and values for some states for the policy found by policy iteration:-
s = (1, 0, 9) : pi*[s] = (-1, -1) , V*[s] = -186.12579511
s = (2, 2, 0) : pi*[s] = (1, -1) , V*[s] = 4.3046721
s = (7, 4, 1) : pi*[s] = (1, 1) , V*[s] = -94.09509999999999
s = (3, 2, 11) : pi*[s] = (1, -1) , V*[s] = 4.7829690000000005
s = (5, 4, 7) : pi*[s] = (-1, -1) , V*[s] = 9.0

Simulating 200  time steps...
Cumulative reward for the trajectory just shown: 3.874204882944923

Initializing the MDP with new reward function and p_error = 0.25 ...
................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................
Done!

Performing policy iteration...
..........
Actions and values for some states for the policy found by policy iteration:-
s = (3, 0, 9) : pi*[s] = (1, -1) , V*[s] = -204.2598209504697
s = (1, 5, 11) : pi*[s] = (-1, 0) , V*[s] = 1.5341388590947294
s = (0, 5, 7) : pi*[s] = (-1, 1) , V*[s] = -140.32314285488775
s = (2, 4, 6) : pi*[s] = (1, -1) , V*[s] = 2.1952081270174735
s = (5, 2, 9) : pi*[s] = (1, -1) , V*[s] = 3.1590443140068434

Simulating 200  time steps...
Cumulative reward for the trajectory just shown: -0.6472014898454218

(Q 5c)
Conclusions:-